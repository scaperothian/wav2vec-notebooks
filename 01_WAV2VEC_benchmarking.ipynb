{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d89304a",
   "metadata": {},
   "source": [
    "# WAV2VEC2 Inference Benchmarking\n",
    "\n",
    "Based on https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2185b3",
   "metadata": {},
   "source": [
    "## Pytorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae49473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n",
      "PyTorch Version: 1.12.0, Pytorchaudio Version: 0.12.0, Targeted Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# was recommended for an error with mps mode (didn't seem to help).  \n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1 \n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}, Pytorchaudio Version: {torchaudio.__version__}, Targeted Device: {device}\")\n",
    "\n",
    "#SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "#SPEECH_FILE = \"_assets/speech.wav\"\n",
    "basename = \"data/mary_had_a_little_lamb_spoken\"\n",
    "SPEECH_FILE = basename + \".wav\"\n",
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08082a1-efa0-4d0e-b432-6672a5f84c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_waveform(data: torch.Tensor,sr: float, seconds: float) ->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Summary: truncates tensor to length that cooresponds to sr * seconds\n",
    "    \n",
    "    Input Arguments: \n",
    "    data (torch.Tensor) - audio data\n",
    "    sr (torch.Float) - sampling rate of audio data\n",
    "    seconds - seconds of data to return. \n",
    "            if the seconds is specified to be larger than the len(tensor) / sr, then an error is raised.  \n",
    "            if seconds is 0, then error is raised.\n",
    "    \n",
    "    Return: torch.Tensor that is truncated from input, or unmodified.\n",
    "    \n",
    "    \"\"\"\n",
    "    # calculate the truncation length of tensor.\n",
    "    truncation_len = int(np.floor(sr * seconds))\n",
    "    \n",
    "    # pull out dimensions.\n",
    "    dim, max_len = data.size()\n",
    "    \n",
    "    if max_len < truncation_len: \n",
    "        raise Exception(f\"Expected seconds to be less than: {max_len / sr}.  Seconds is: {seconds}.\")\n",
    "    \n",
    "    # check for errors in the input.\n",
    "    if seconds == 0.0: \n",
    "        raise Exception(\"seconds cannot be 0.0.\")\n",
    "    \n",
    "    return data[:,:truncation_len]\n",
    "    \n",
    "# tests in this assume the waveform is 5s long.\n",
    "testing_truncate_waveform = False\n",
    "wavey, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "if testing_truncate_waveform:\n",
    "    try: \n",
    "        w1 = truncate_waveform(wavey, sr=sample_rate, seconds=0.0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    try: \n",
    "        w2 = truncate_waveform(wavey, sr=sample_rate, seconds=6.0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try: \n",
    "        w3 = truncate_waveform(wavey, sr=sample_rate, seconds=1.0)\n",
    "        print(w3.size())\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43ed4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavform is: 1.0s long.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(SPEECH_FILE):\n",
    "    max_length_secs = 1.0\n",
    "    waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    sample_rate = bundle.sample_rate\n",
    "    waveform_trunc = truncate_waveform(waveform, sample_rate, seconds=max_length_secs)\n",
    "    waveform = waveform_trunc.to(device)\n",
    "else:\n",
    "    print('NO FILE HERE!')\n",
    "\n",
    "chans, samples = waveform.size()\n",
    "print(f\"wavform is: {samples / bundle.sample_rate}s long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e213f5-4c5c-40bf-bac0-acef601886b8",
   "metadata": {},
   "source": [
    "## Pytorch Inference (CPU) Benchmark\n",
    "**Objective**: Measure single inference lookback to understand if I can keep up real time.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fcf7025-b970-4dfd-84e2-952837e02c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57ccbc0-48c1-44d6-93d6-2c8930a7cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Summary: simple decoder using argmax to determine best character, \n",
    "             then remove duplicates per CTC's algorithm.\n",
    "    \n",
    "    Note: would be better to use CTC using maximumizing liklihood of sequence \n",
    "         (i.e. using adjacent logits to guess characters.) \n",
    "    \"\"\"\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor):\n",
    "        \"\"\"Given a sequence emission over labels, get the best path\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          List[str]: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        joined = \"\".join([self.labels[i] for i in indices])\n",
    "        return joined.replace(\"|\", \" \").strip().split()\n",
    "\n",
    "tokens = [label.lower() for label in bundle.get_labels()]\n",
    "greedy_decoder = GreedyCTCDecoder(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42c833bb-dfb4-443e-9817-6f811ba8a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference (with decoding): 0.4526517391204834\n",
      "Transcript: ['mary', 'had', 'a']\n"
     ]
    }
   ],
   "source": [
    "# benchmark for 1.0s\n",
    "start = time.time()\n",
    "with torch.inference_mode():\n",
    "  emission, _ = model(waveform)\n",
    "  transcript = greedy_decoder(emission[0])\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference (with decoding): {finish-start}\")\n",
    "print(f\"Transcript: {transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b8b74-cd94-49a2-b3fb-3c2eb530a03a",
   "metadata": {},
   "source": [
    "**Conclusion**: CPU based inference is not good enough a realtime lookback.  because the delays are going to make predicting words practically, not useful.  Need a response time near 100ms max (speculation).  \n",
    "\n",
    "Question: I believe the model emissions / logits will perform better if there is more history.  With this in mind, can I actually just concatenate prior emissions with a good CTC decoder?\n",
    "\n",
    "COA 1: concatenate emissions, perform decoding every pass on longer array of emissions.<br>\n",
    "COA 2: concatenate emissions, perform decoding every pass on longer array of emissions AND add a LM that can correct for issues in text.<br>\n",
    "COA 3: make a guess at what will be needed 1 second into the future (bad idea).<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d602139-cc71-419a-80c5-14dd4e4c2145",
   "metadata": {},
   "source": [
    "# Pytorch Inference (M1 GPU) Benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63d177c6-8491-4cb6-a3e4-749ad42d0e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device   = torch.device('mps')\n",
    "waveform = waveform_trunc.to(device)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b84a2f69-00f9-4814-a493-e9b671e742eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/das/opt/anaconda3/envs/mldev/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:24: UserWarning: The operator 'aten::_weight_norm_interface' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  return _weight_norm(v, g, self.dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference (with decoding): 0.7504770755767822\n",
      "Transcript: []\n"
     ]
    }
   ],
   "source": [
    "# benchmark for 1.0s\n",
    "start = time.time()\n",
    "with torch.inference_mode():\n",
    "  emission, _ = model(waveform[0,:].reshape(1,16000))\n",
    "  transcript = greedy_decoder(emission[0])\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference (with decoding): {finish-start}\")\n",
    "print(f\"Transcript: {transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ece6e-7ec1-43db-8a88-f728bd9b240c",
   "metadata": {},
   "source": [
    "**Conclusion**: GPU support for weight norm is cuasing some kind of issue.  I used the FALLBACK (see above) environmental variable to see if that helped, but the response is blank, so Im not sure if the weight norm is doing a pass through thing, and then corrupting the results, or if the data is not making it to the output for some reason.  This issue may be similar to the CoreML issue below.\n",
    "\n",
    "Issue here: \n",
    "https://github.com/pytorch/pytorch/issues/77764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd3962-df30-4258-9cec-83df582ae4c3",
   "metadata": {},
   "source": [
    "# ONNX Benchmark\n",
    "**Objective**: Attempt to see if ONNX gives you some imporovement in performance on CPU\n",
    "https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f7b1cfb-07bb-4ada-b2a7-ad8f544e0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device('cpu')\n",
    "waveform = waveform_trunc.to(device)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9444a56-187e-4766-ad1e-651fc8ce8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/das/opt/anaconda3/envs/mldev/lib/python3.9/site-packages/torch/nn/functional.py:2515: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX file saved.\n"
     ]
    }
   ],
   "source": [
    "import onnx, onnxruntime\n",
    "\n",
    "# Exporting Pytorch Model to ONNX file...\n",
    "\n",
    "# Putting into eval mode: i.e. disabling calculation of gradients.\n",
    "model.eval()\n",
    "\n",
    "# Adding example inputs to perform something called tracing...\n",
    "example_input = torch.randn(1,16000, requires_grad=True)\n",
    "# returns example emissions (garbage output) from the pytorch model\n",
    "torch_out = model(example_input)\n",
    "\n",
    "# take the original pytorch model, and the example_input and export to file...\n",
    "torch.onnx.export(model,example_input,'wav2vec2.onnx',export_params=True)\n",
    "\n",
    "print(\"ONNX file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3cf0cf1-6ec1-4817-ae6a-ab06e3054090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Ready for Inference\n"
     ]
    }
   ],
   "source": [
    "# Loading ONNX file and preparing for Inference.\n",
    "\n",
    "# loading the model into memory, checking for issues, c\n",
    "omodel = onnx.load('wav2vec2.onnx')\n",
    "onnx.checker.check_model(omodel)\n",
    "\n",
    "# InferenceSession is the main class of ONNX Runtime. \n",
    "# It is used to load and run an ONNX model, \n",
    "# as well as specify environment and application configuration options.\n",
    "# as far as I can tell GPU support is spotty for M1\n",
    "# https://onnxruntime.ai/docs/execution-providers/\n",
    "ort_session = onnxruntime.InferenceSession('wav2vec2.onnx')\n",
    "print(\"Model Ready for Inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5a3753b-6259-4ce1-b2a9-5f02fabff1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference: 0.14493918418884277\n",
      "['mary', 'had', 'a']\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(waveform[0,:].reshape(1,16000))}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "transcript = greedy_decoder(torch.Tensor(ort_outs[0][0]))\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference: {finish-start}\")\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff500c-c66f-44e8-b1b3-a9d9704dcdc3",
   "metadata": {},
   "source": [
    "**Conclusion**: ONNX does some nice improvements!!  Will use that for real time inference.  Just give myself some intuition for how often 0.2s delay in inference feels like, the below printout updates every 0.2s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65101a52-986c-4187-ab8c-87c1efc86c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "i = 0\n",
    "while True: \n",
    "    print(f\"Hello: {i}\")\n",
    "    i += 1\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfae13-d1ed-4ef4-aee4-68c686a4d76f",
   "metadata": {},
   "source": [
    "# CoreML Benchmark\n",
    "\n",
    "Pytorch directly to CoreML:<br>\n",
    "https://coremltools.readme.io/docs/pytorch-conversion#generate-a-torchscript-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae833edf-f529-4d5c-b9c4-4fd6c5161a4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Exporting Pytorch Model to CoreML file...\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Putting into eval mode: i.e. disabling calculation of gradients.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Adding example inputs to perform something called tracing...\u001b[39;00m\n\u001b[1;32m      7\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m16000\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Exporting Pytorch Model to CoreML file...\n",
    "\n",
    "# Putting into eval mode: i.e. disabling calculation of gradients.\n",
    "model.eval()\n",
    "\n",
    "# Adding example inputs to perform something called tracing...\n",
    "example_input = torch.randn(1,16000, requires_grad=True)\n",
    "try:\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acecee5-a703-459b-90ce-1b1e68ff8721",
   "metadata": {},
   "source": [
    "**Conclusion**: there is some issue with weight norm that I think there is a work around for, but I haven't have the time to work through it. <br>\n",
    "https://github.com/pytorch/pytorch/issues/57289\n",
    "\n",
    "Apparently, this may work:<br> \n",
    "`for layer in layers_with_weight_norm:`<br>\n",
    "`   torch.nn.utils.remove_weight_norm(layer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16b103-b578-41e4-bfa5-474bde8b39f9",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Attempt to profile the ONNX model.<br>\n",
    "Is this running on the M1 chip? <br>\n",
    "\n",
    "Here is an example for profiling:<br>\n",
    "https://machinelearning.apple.com/research/neural-engine-transformers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f02c5-9b3d-4250-b58f-742c1bf7439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldev",
   "language": "python",
   "name": "mldev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
