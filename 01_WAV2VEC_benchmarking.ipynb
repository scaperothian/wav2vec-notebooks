{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d89304a",
   "metadata": {},
   "source": [
    "# WAV2VEC2 Inference Benchmarking\n",
    "\n",
    "Based on https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2185b3",
   "metadata": {},
   "source": [
    "## Pytorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae49473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.1, Pytorchaudio Version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "# was recommended for an error with mps mode (didn't seem to help).  \n",
    "# %env PYTORCH_ENABLE_MPS_FALLBACK=1 \n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "print(f\"PyTorch Version: {torch.__version__}, Pytorchaudio Version: {torchaudio.__version__}\")\n",
    "\n",
    "#SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "#SPEECH_FILE = \"_assets/speech.wav\"\n",
    "basename = \"data/mary_had_a_little_lamb_spoken\"\n",
    "SPEECH_FILE = basename + \".wav\"\n",
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08082a1-efa0-4d0e-b432-6672a5f84c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "seconds cannot be 0.0.\n",
      "Test 2\n",
      "Expected seconds to be less than: 22.24.  Seconds is: 30.0.\n",
      "Test 3\n",
      "torch.Size([1, 44100])\n"
     ]
    }
   ],
   "source": [
    "def truncate_waveform(data: torch.Tensor,sr: float, seconds: float) ->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Summary: truncates tensor to length that cooresponds to sr * seconds\n",
    "    \n",
    "    Input Arguments: \n",
    "    data (torch.Tensor) - audio data\n",
    "    sr (torch.Float) - sampling rate of audio data\n",
    "    seconds - seconds of data to return. \n",
    "            if the seconds is specified to be larger than the len(tensor) / sr, then an error is raised.  \n",
    "            if seconds is 0, then error is raised.\n",
    "    \n",
    "    Return: torch.Tensor that is truncated from input, or unmodified.\n",
    "    \n",
    "    \"\"\"\n",
    "    # calculate the truncation length of tensor.\n",
    "    truncation_len = int(np.floor(sr * seconds))\n",
    "    \n",
    "    # pull out dimensions.\n",
    "    dim, max_len = data.shape\n",
    "    \n",
    "    if max_len < truncation_len: \n",
    "        raise Exception(f\"Expected seconds to be less than: {max_len / sr:.2f}.  Seconds is: {seconds}.\")\n",
    "    \n",
    "    # check for errors in the input.\n",
    "    if seconds == 0.0: \n",
    "        raise Exception(\"seconds cannot be 0.0.\")\n",
    "\n",
    "    #if max_len > truncation_len:\n",
    "    #    print(\"normal truncation occuring.\") \n",
    "    \n",
    "    return data[:,:truncation_len]\n",
    "    \n",
    "# tests in this assume the waveform is 5s long.\n",
    "testing_truncate_waveform = True\n",
    "wavey, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "if testing_truncate_waveform:\n",
    "    print('Test 1')\n",
    "    try: \n",
    "        w1 = truncate_waveform(wavey, sr=sample_rate, seconds=0.0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print('Test 2')\n",
    "    try: \n",
    "        w2 = truncate_waveform(wavey, sr=sample_rate, seconds=30.0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print('Test 3')\n",
    "    try: \n",
    "        w3 = truncate_waveform(wavey, sr=sample_rate, seconds=1.0)\n",
    "        print(w3.size())\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e213f5-4c5c-40bf-bac0-acef601886b8",
   "metadata": {},
   "source": [
    "## Pytorch Inference (CPU) Benchmark\n",
    "**Objective**: Measure single inference lookback to understand if I can keep up real time.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57569d28-6e77-4558-b96b-9f6bc31b7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "device = torch.device('mps')\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ed4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sample Rate: 16000\n",
      "wavform is: 1.0s long (16000 samples).\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(SPEECH_FILE):\n",
    "    max_length_secs = 1.0\n",
    "    waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    sample_rate = bundle.sample_rate\n",
    "    waveform_trunc = truncate_waveform(waveform, sample_rate, seconds=max_length_secs)\n",
    "    waveform = waveform_trunc.to(device)\n",
    "    print(f\"Target Sample Rate: {sample_rate}\")\n",
    "else:\n",
    "    print('NO FILE HERE!')\n",
    "\n",
    "chans, samples = waveform.size()\n",
    "print(f\"wavform is: {samples / bundle.sample_rate}s long ({samples} samples).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acdc4da2-b913-4fdd-8437-0ab6664c5384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,16000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57ccbc0-48c1-44d6-93d6-2c8930a7cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Summary: simple decoder using argmax to determine best character, \n",
    "             then remove duplicates per CTC's algorithm.\n",
    "    \n",
    "    Note: would be better to use CTC using maximumizing liklihood of sequence \n",
    "         (i.e. using adjacent logits to guess characters.) \n",
    "    \"\"\"\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor):\n",
    "        \"\"\"Given a sequence emission over labels, get the best path\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          List[str]: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        joined = \"\".join([self.labels[i] for i in indices])\n",
    "        return joined.replace(\"|\", \" \").strip().split()\n",
    "\n",
    "tokens = [label.lower() for label in bundle.get_labels()]\n",
    "greedy_decoder = GreedyCTCDecoder(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42c833bb-dfb4-443e-9817-6f811ba8a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference (with decoding): 0.08699202537536621\n",
      "Transcript: ['mary', 'had', 'a']\n"
     ]
    }
   ],
   "source": [
    "# benchmark for 1.0s\n",
    "start = time.time()\n",
    "with torch.inference_mode():\n",
    "  emission, _ = model(waveform)\n",
    "  transcript = greedy_decoder(emission[0])\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference (with decoding): {finish-start}\")\n",
    "print(f\"Transcript: {transcript}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be0e8ac5-e424-4eff-9162-a9a010544feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 29])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b8b74-cd94-49a2-b3fb-3c2eb530a03a",
   "metadata": {},
   "source": [
    "**Conclusion**: CPU based inference is not good enough a realtime lookback.  because the delays are going to make predicting words practically, not useful.  Need a response time near 100ms max (speculation).  \n",
    "\n",
    "Question: I believe the model emissions / logits will perform better if there is more history.  With this in mind, can I actually just concatenate prior emissions with a good CTC decoder?\n",
    "\n",
    "COA 1: concatenate emissions, perform decoding every pass on longer array of emissions.<br>\n",
    "COA 2: concatenate emissions, perform decoding every pass on longer array of emissions AND add a LM that can correct for issues in text.<br>\n",
    "COA 3: make a guess at what will be needed 1 second into the future (bad idea).<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d602139-cc71-419a-80c5-14dd4e4c2145",
   "metadata": {},
   "source": [
    "# Pytorch Inference (M1 GPU) Benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63d177c6-8491-4cb6-a3e4-749ad42d0e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device   = torch.device('mps')\n",
    "waveform = waveform_trunc.to(device)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b84a2f69-00f9-4814-a493-e9b671e742eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference (with decoding): 0.23835325241088867\n",
      "Transcript: ['mary', 'had', 'a']\n"
     ]
    }
   ],
   "source": [
    "# benchmark for 1.0s\n",
    "start = time.time()\n",
    "transcript_gpu = ''\n",
    "try: \n",
    "    with torch.inference_mode():\n",
    "        emission, _ = model(waveform[0,:].reshape(1,16000))\n",
    "        transcript_gpu = greedy_decoder(emission[0])\n",
    "\n",
    "except NotImplementedError as e:\n",
    "    print(\"NotImplementedError:\",e)\n",
    "\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference (with decoding): {finish-start}\")\n",
    "print(f\"Transcript: {transcript_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ece6e-7ec1-43db-8a88-f728bd9b240c",
   "metadata": {},
   "source": [
    "**Conclusion**: GPU support for weight norm is cuasing some kind of issue.  I used the FALLBACK (see above) environmental variable to see if that helped, but the response is blank, so Im not sure if the weight norm is doing a pass through thing, and then corrupting the results, or if the data is not making it to the output for some reason.  This issue may be similar to the CoreML issue below.\n",
    "\n",
    "Issue here: \n",
    "https://github.com/pytorch/pytorch/issues/77764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfae13-d1ed-4ef4-aee4-68c686a4d76f",
   "metadata": {},
   "source": [
    "# CoreML Benchmark\n",
    "\n",
    "Pytorch directly to CoreML:<br>\n",
    "https://coremltools.readme.io/docs/pytorch-conversion#generate-a-torchscript-version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acecee5-a703-459b-90ce-1b1e68ff8721",
   "metadata": {},
   "source": [
    "**Conclusion**: there is some issue with weight norm that I think there is a work around for, but I haven't have the time to work through it. <br>\n",
    "https://github.com/pytorch/pytorch/issues/57289\n",
    "\n",
    "Apparently, this may work:<br> \n",
    "`for layer in layers_with_weight_norm:`<br>\n",
    "`   torch.nn.utils.remove_weight_norm(layer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd3962-df30-4258-9cec-83df582ae4c3",
   "metadata": {},
   "source": [
    "# ONNX Benchmark\n",
    "**Objective**: Attempt to see if ONNX gives you some imporovement in performance on CPU\n",
    "https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f7b1cfb-07bb-4ada-b2a7-ad8f544e0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device('cpu')\n",
    "waveform = waveform_trunc.to(device)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9444a56-187e-4766-ad1e-651fc8ce8725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX file saved.\n"
     ]
    }
   ],
   "source": [
    "import onnx, onnxruntime\n",
    "\n",
    "# Exporting Pytorch Model to ONNX file...\n",
    "\n",
    "# Putting into eval mode: i.e. disabling calculation of gradients.\n",
    "model.eval()\n",
    "\n",
    "# Adding example inputs to perform something called tracing...\n",
    "example_input = torch.randn(1,16000)\n",
    "# returns example emissions (garbage output) from the pytorch model\n",
    "torch_out = model(example_input)\n",
    "\n",
    "# take the original pytorch model, and the example_input and export to file...\n",
    "torch.onnx.export(model,\n",
    "                  example_input,\n",
    "                  'wav2vec2.onnx')\n",
    "\n",
    "print(\"ONNX file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3cf0cf1-6ec1-4817-ae6a-ab06e3054090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Ready for Inference\n"
     ]
    }
   ],
   "source": [
    "# Loading ONNX file and preparing for Inference.\n",
    "\n",
    "# loading the model into memory, checking for issues, c\n",
    "omodel = onnx.load('wav2vec2.onnx')\n",
    "onnx.checker.check_model(omodel)\n",
    "\n",
    "# InferenceSession is the main class of ONNX Runtime. \n",
    "# It is used to load and run an ONNX model, \n",
    "# as well as specify environment and application configuration options.\n",
    "# as far as I can tell GPU support is spotty for M1\n",
    "# https://onnxruntime.ai/docs/execution-providers/\n",
    "ort_session = onnxruntime.InferenceSession('wav2vec2.onnx')\n",
    "print(\"Model Ready for Inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5a3753b-6259-4ce1-b2a9-5f02fabff1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference: 0.09635710716247559\n",
      "['mary', 'had', 'a']\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(waveform[0,:].reshape(1,16000))}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "transcript = greedy_decoder(torch.Tensor(ort_outs[0][0]))\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference: {finish-start}\")\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff500c-c66f-44e8-b1b3-a9d9704dcdc3",
   "metadata": {},
   "source": [
    "**Conclusion**: ONNX used to work, but with the latest update there are some holes that haven't been filled, so I'm going to ignore ONNX until I absolutely need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65101a52-986c-4187-ab8c-87c1efc86c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "i = 0\n",
    "while True: \n",
    "    print(f\"Hello: {i}\")\n",
    "    i += 1\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16b103-b578-41e4-bfa5-474bde8b39f9",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Attempt to profile the ONNX model.<br>\n",
    "Is this running on the M1 chip? <br>\n",
    "\n",
    "Here is an example for profiling:<br>\n",
    "https://machinelearning.apple.com/research/neural-engine-transformers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f02c5-9b3d-4250-b58f-742c1bf7439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WAV2VEC (with Poetry)",
   "language": "python",
   "name": "wav2vec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
