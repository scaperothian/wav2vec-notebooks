{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d89304a",
   "metadata": {},
   "source": [
    "# WAV2VEC2 Inference Benchmarking\n",
    "\n",
    "Based on https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2185b3",
   "metadata": {},
   "source": [
    "## Pytorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae49473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.0.1, Pytorchaudio Version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "# was recommended for an error with mps mode (didn't seem to help).  \n",
    "# %env PYTORCH_ENABLE_MPS_FALLBACK=1 \n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "print(f\"PyTorch Version: {torch.__version__}, Pytorchaudio Version: {torchaudio.__version__}\")\n",
    "\n",
    "#SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "#SPEECH_FILE = \"_assets/speech.wav\"\n",
    "basename = \"data/mary_had_a_little_lamb_spoken\"\n",
    "SPEECH_FILE = basename + \".wav\"\n",
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08082a1-efa0-4d0e-b432-6672a5f84c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "seconds cannot be 0.0.\n",
      "Test 2\n",
      "Expected seconds to be less than: 22.24.  Seconds is: 30.0.\n",
      "Test 3\n",
      "torch.Size([1, 44100])\n"
     ]
    }
   ],
   "source": [
    "def truncate_waveform(data: torch.Tensor,sr: float, seconds: float) ->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Summary: truncates tensor to length that cooresponds to sr * seconds\n",
    "    \n",
    "    Input Arguments: \n",
    "    data (torch.Tensor) - audio data\n",
    "    sr (torch.Float) - sampling rate of audio data\n",
    "    seconds - seconds of data to return. \n",
    "            if the seconds is specified to be larger than the len(tensor) / sr, then an error is raised.  \n",
    "            if seconds is 0, then error is raised.\n",
    "    \n",
    "    Return: torch.Tensor that is truncated from input, or unmodified.\n",
    "    \n",
    "    \"\"\"\n",
    "    # calculate the truncation length of tensor.\n",
    "    truncation_len = int(np.floor(sr * seconds))\n",
    "    \n",
    "    # pull out dimensions.\n",
    "    dim, max_len = data.shape\n",
    "    \n",
    "    if max_len < truncation_len: \n",
    "        raise Exception(f\"Expected seconds to be less than: {max_len / sr:.2f}.  Seconds is: {seconds}.\")\n",
    "    \n",
    "    # check for errors in the input.\n",
    "    if seconds == 0.0: \n",
    "        raise Exception(\"seconds cannot be 0.0.\")\n",
    "\n",
    "    #if max_len > truncation_len:\n",
    "    #    print(\"normal truncation occuring.\") \n",
    "    \n",
    "    return data[:,:truncation_len]\n",
    "    \n",
    "# tests in this assume the waveform is 5s long.\n",
    "testing_truncate_waveform = True\n",
    "wavey, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "if testing_truncate_waveform:\n",
    "    print('Test 1')\n",
    "    try: \n",
    "        w1 = truncate_waveform(wavey, sr=sample_rate, seconds=0.0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print('Test 2')\n",
    "    try: \n",
    "        w2 = truncate_waveform(wavey, sr=sample_rate, seconds=30.0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print('Test 3')\n",
    "    try: \n",
    "        w3 = truncate_waveform(wavey, sr=sample_rate, seconds=1.0)\n",
    "        print(w3.size())\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e213f5-4c5c-40bf-bac0-acef601886b8",
   "metadata": {},
   "source": [
    "## Pytorch Inference (CPU) Benchmark\n",
    "**Objective**: Measure single inference lookback to understand if I can keep up real time.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57569d28-6e77-4558-b96b-9f6bc31b7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "device = torch.device('cpu')\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ed4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavform is: 1.0s long.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(SPEECH_FILE):\n",
    "    max_length_secs = 1.0\n",
    "    waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    sample_rate = bundle.sample_rate\n",
    "    waveform_trunc = truncate_waveform(waveform, sample_rate, seconds=max_length_secs)\n",
    "    waveform = waveform_trunc.to(device)\n",
    "else:\n",
    "    print('NO FILE HERE!')\n",
    "\n",
    "chans, samples = waveform.size()\n",
    "print(f\"wavform is: {samples / bundle.sample_rate}s long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57ccbc0-48c1-44d6-93d6-2c8930a7cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Summary: simple decoder using argmax to determine best character, \n",
    "             then remove duplicates per CTC's algorithm.\n",
    "    \n",
    "    Note: would be better to use CTC using maximumizing liklihood of sequence \n",
    "         (i.e. using adjacent logits to guess characters.) \n",
    "    \"\"\"\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor):\n",
    "        \"\"\"Given a sequence emission over labels, get the best path\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          List[str]: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        joined = \"\".join([self.labels[i] for i in indices])\n",
    "        return joined.replace(\"|\", \" \").strip().split()\n",
    "\n",
    "tokens = [label.lower() for label in bundle.get_labels()]\n",
    "greedy_decoder = GreedyCTCDecoder(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c833bb-dfb4-443e-9817-6f811ba8a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform inference (with decoding): 0.07058501243591309\n",
      "Transcript: ['mary', 'had', 'a']\n"
     ]
    }
   ],
   "source": [
    "# benchmark for 1.0s\n",
    "start = time.time()\n",
    "with torch.inference_mode():\n",
    "  emission, _ = model(waveform)\n",
    "  transcript = greedy_decoder(emission[0])\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference (with decoding): {finish-start}\")\n",
    "print(f\"Transcript: {transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b8b74-cd94-49a2-b3fb-3c2eb530a03a",
   "metadata": {},
   "source": [
    "**Conclusion**: CPU based inference is not good enough a realtime lookback.  because the delays are going to make predicting words practically, not useful.  Need a response time near 100ms max (speculation).  \n",
    "\n",
    "Question: I believe the model emissions / logits will perform better if there is more history.  With this in mind, can I actually just concatenate prior emissions with a good CTC decoder?\n",
    "\n",
    "COA 1: concatenate emissions, perform decoding every pass on longer array of emissions.<br>\n",
    "COA 2: concatenate emissions, perform decoding every pass on longer array of emissions AND add a LM that can correct for issues in text.<br>\n",
    "COA 3: make a guess at what will be needed 1 second into the future (bad idea).<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d602139-cc71-419a-80c5-14dd4e4c2145",
   "metadata": {},
   "source": [
    "# Pytorch Inference (M1 GPU) Benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d177c6-8491-4cb6-a3e4-749ad42d0e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device   = torch.device('mps')\n",
    "waveform = waveform_trunc.to(device)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84a2f69-00f9-4814-a493-e9b671e742eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotImplementedError: The operator 'aten::_weight_norm_interface' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "Time to perform inference (with decoding): 0.11345505714416504\n",
      "Transcript: \n"
     ]
    }
   ],
   "source": [
    "# benchmark for 1.0s\n",
    "start = time.time()\n",
    "transcript_gpu = ''\n",
    "try: \n",
    "    with torch.inference_mode():\n",
    "        emission, _ = model(waveform[0,:].reshape(1,16000))\n",
    "        transcript_gpu = greedy_decoder(emission[0])\n",
    "\n",
    "except NotImplementedError as e:\n",
    "    print(\"NotImplementedError:\",e)\n",
    "\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference (with decoding): {finish-start}\")\n",
    "print(f\"Transcript: {transcript_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ece6e-7ec1-43db-8a88-f728bd9b240c",
   "metadata": {},
   "source": [
    "**Conclusion**: GPU support for weight norm is cuasing some kind of issue.  I used the FALLBACK (see above) environmental variable to see if that helped, but the response is blank, so Im not sure if the weight norm is doing a pass through thing, and then corrupting the results, or if the data is not making it to the output for some reason.  This issue may be similar to the CoreML issue below.\n",
    "\n",
    "Issue here: \n",
    "https://github.com/pytorch/pytorch/issues/77764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfae13-d1ed-4ef4-aee4-68c686a4d76f",
   "metadata": {},
   "source": [
    "# CoreML Benchmark\n",
    "\n",
    "Pytorch directly to CoreML:<br>\n",
    "https://coremltools.readme.io/docs/pytorch-conversion#generate-a-torchscript-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae833edf-f529-4d5c-b9c4-4fd6c5161a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'WeightNorm' object has no attribute '__name__'\n"
     ]
    }
   ],
   "source": [
    "# Exporting Pytorch Model to CoreML file...\n",
    "\n",
    "# Putting into eval mode: i.e. disabling calculation of gradients.\n",
    "model.eval()\n",
    "\n",
    "# Adding example inputs to perform something called tracing...\n",
    "example_input = torch.randn(1,16000, requires_grad=True)\n",
    "try:\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acecee5-a703-459b-90ce-1b1e68ff8721",
   "metadata": {},
   "source": [
    "**Conclusion**: there is some issue with weight norm that I think there is a work around for, but I haven't have the time to work through it. <br>\n",
    "https://github.com/pytorch/pytorch/issues/57289\n",
    "\n",
    "Apparently, this may work:<br> \n",
    "`for layer in layers_with_weight_norm:`<br>\n",
    "`   torch.nn.utils.remove_weight_norm(layer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd3962-df30-4258-9cec-83df582ae4c3",
   "metadata": {},
   "source": [
    "# ONNX Benchmark\n",
    "**Objective**: Attempt to see if ONNX gives you some imporovement in performance on CPU\n",
    "https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7b1cfb-07bb-4ada-b2a7-ad8f544e0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device('cpu')\n",
    "waveform = waveform_trunc.to(device)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9444a56-187e-4766-ad1e-651fc8ce8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 1 ERROR ========================\n",
      "ERROR: missing-standard-symbolic-function\n",
      "=========================================\n",
      "Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.\n",
      "None\n",
      "<Set verbose=True to see more details>\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m torch_out \u001b[38;5;241m=\u001b[39m model(example_input)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# take the original pytorch model, and the example_input and export to file...\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwav2vec2.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX file saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchaudio/lib/python3.8/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchaudio/lib/python3.8/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1563\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1564\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchaudio/lib/python3.8/site-packages/torch/onnx/utils.py:1117\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1128\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchaudio/lib/python3.8/site-packages/torch/onnx/utils.py:665\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    662\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    663\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 665\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    667\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchaudio/lib/python3.8/site-packages/torch/onnx/utils.py:1901\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1898\u001b[0m         \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context\u001b[38;5;241m.\u001b[39mop(op_name, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattrs, outputs\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39moutputsSize())  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1902\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1903\u001b[0m         opset_version,\n\u001b[1;32m   1904\u001b[0m         symbolic_function_group\u001b[38;5;241m.\u001b[39mget_min_supported()\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1906\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1907\u001b[0m     )\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m operator_export_type \u001b[38;5;241m==\u001b[39m _C_onnx\u001b[38;5;241m.\u001b[39mOperatorExportTypes\u001b[38;5;241m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues."
     ]
    }
   ],
   "source": [
    "import onnx, onnxruntime\n",
    "\n",
    "# Exporting Pytorch Model to ONNX file...\n",
    "\n",
    "# Putting into eval mode: i.e. disabling calculation of gradients.\n",
    "model.eval()\n",
    "\n",
    "# Adding example inputs to perform something called tracing...\n",
    "example_input = torch.randn(1,16000, requires_grad=True)\n",
    "# returns example emissions (garbage output) from the pytorch model\n",
    "torch_out = model(example_input)\n",
    "\n",
    "# take the original pytorch model, and the example_input and export to file...\n",
    "torch.onnx.export(model,example_input,'wav2vec2.onnx',export_params=True)\n",
    "\n",
    "print(\"ONNX file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf0cf1-6ec1-4817-ae6a-ab06e3054090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ONNX file and preparing for Inference.\n",
    "\n",
    "# loading the model into memory, checking for issues, c\n",
    "omodel = onnx.load('wav2vec2.onnx')\n",
    "onnx.checker.check_model(omodel)\n",
    "\n",
    "# InferenceSession is the main class of ONNX Runtime. \n",
    "# It is used to load and run an ONNX model, \n",
    "# as well as specify environment and application configuration options.\n",
    "# as far as I can tell GPU support is spotty for M1\n",
    "# https://onnxruntime.ai/docs/execution-providers/\n",
    "ort_session = onnxruntime.InferenceSession('wav2vec2.onnx')\n",
    "print(\"Model Ready for Inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3753b-6259-4ce1-b2a9-5f02fabff1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start= time.time()\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(waveform[0,:].reshape(1,16000))}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "transcript = greedy_decoder(torch.Tensor(ort_outs[0][0]))\n",
    "finish = time.time()\n",
    "print(f\"Time to perform inference: {finish-start}\")\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff500c-c66f-44e8-b1b3-a9d9704dcdc3",
   "metadata": {},
   "source": [
    "**Conclusion**: ONNX used to work, but with the latest update there are some holes that haven't been filled, so I'm going to ignore ONNX until I absolutely need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65101a52-986c-4187-ab8c-87c1efc86c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "i = 0\n",
    "while True: \n",
    "    print(f\"Hello: {i}\")\n",
    "    i += 1\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16b103-b578-41e4-bfa5-474bde8b39f9",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Attempt to profile the ONNX model.<br>\n",
    "Is this running on the M1 chip? <br>\n",
    "\n",
    "Here is an example for profiling:<br>\n",
    "https://machinelearning.apple.com/research/neural-engine-transformers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f02c5-9b3d-4250-b58f-742c1bf7439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchaudio",
   "language": "python",
   "name": "pytorchaudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
